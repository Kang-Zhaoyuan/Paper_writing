\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{left=1in,right=0.75in,top=1in,bottom=1in,headheight=14.49998pt}

\newcommand{\Problem}{C}
\newcommand{\Team}{2627882}
\newcommand{\Title}{Balancing Judges and Fans: Reconstructing Votes and Evaluating DWTS Scoring Rules}

\usepackage{fontspec}
\setmainfont{Times New Roman}


\usepackage{indentfirst}
\setlength{\parindent}{2em}
\setlength{\parskip}{0pt}
\usepackage{csquotes}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{flafter}
\usepackage[section]{placeins}
\usepackage{xcolor}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage{lastpage}
\usepackage{listings}


% (Hyperlinks disabled per requirements; remove hyperref/url to avoid clickable links)

% Search images under ./images by default
\graphicspath{{images/}}

% Safely include graphics: show a placeholder box if the file is missing
\makeatletter
\let\orig@includegraphics\includegraphics
\newcommand{\safeincludegraphics}[2][]{%
	\begingroup
	\def\temp@file{#2}%
	\IfFileExists{\temp@file}{\orig@includegraphics[#1]{\temp@file}}{%
		\IfFileExists{\temp@file.pdf}{\orig@includegraphics[#1]{\temp@file.pdf}}{%
			\IfFileExists{\temp@file.png}{\orig@includegraphics[#1]{\temp@file.png}}{%
				\IfFileExists{\temp@file.jpg}{\orig@includegraphics[#1]{\temp@file.jpg}}{%
					\fbox{\parbox[c][4cm][c]{0.9\textwidth}{\centering Missing image: \texttt{#2}}}%
				}}}}%
	\endgroup
}
\renewcommand{\includegraphics}[2][]{\safeincludegraphics[#1]{#2}}
\makeatother

% Float placement tuning
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.9}
\renewcommand{\textfraction}{0.07}
\renewcommand{\floatpagefraction}{0.8}

% Force figures to appear near their source location
\makeatletter
\renewenvironment{figure}[1][]{\@float{figure}[H]}{\end@float}
\makeatother

\renewcommand{\contentsname}{Contents}
\renewcommand{\refname}{References}

\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cftsubsecleader}{\cftdotfill{\cftdotsep}}
\setlength{\cftbeforesecskip}{0.5em}
\setlength{\cftbeforesubsecskip}{0.3em}
\setcounter{tocdepth}{2} % hide subsubsection in TOC

\pagestyle{fancy}
\fancyhf{} 
\lhead{Team Number \Team}
\rhead{Page \thepage\ of \pageref{LastRegularPage}} 
\renewcommand{\headrulewidth}{0.4pt}

\begin{document}

\thispagestyle{empty}
\vspace*{-16ex}
\centerline{
	\begin{tabular}{*3{c}}
		\parbox[t]{0.3\linewidth}{\begin{center}\textbf{Problem Chosen}\\ \Large \textcolor{red}{\Problem}\end{center}} & 
		\parbox[t]{0.3\linewidth}{\begin{center}\textbf{2026\\ MCM/ICM\\ Summary Sheet}\end{center}} & 
		\parbox[t]{0.3\linewidth}{\begin{center}\textbf{Team Control Number}\\ \Large \textcolor{red}{\Team}\end{center}} \\
		\hline
	\end{tabular}
}
\vspace{2ex}

\begin{center}
	\large \textbf{\Title}
\end{center}

\begin{center}
	\textbf{\large Summary}
\end{center}

\begin{sloppypar}

\textit{Dancing with the Stars} (DWTS) has long faced a balancing challenge between \textbf{professional judges' scores} and \textbf{audience votes}. Because fan votes are not publicly released, quantifying audience behavior and assessing rule fairness are difficult. This study follows a \textbf{``data estimation--rule comparison--system design''} pipeline: we infer weekly fan votes via \textbf{Monte Carlo inversion}, replay alternative rules to measure bias and stability, and propose the \textbf{Dual-Track Linear Scoring System (DTLSS)} as a reform recommendation.


\textbf{For Problem 1}, we build a hybrid model combining \textbf{Monte Carlo simulation} and \textbf{parameter inversion}, decomposing votes into \textbf{base votes} and \textbf{performance votes}. The model reproduces historical eliminations with accuracy $\ge 75\%$ in \textbf{31/34} seasons, averaging \textbf{82.3\%}. External validation with \textbf{Google Trends} shows a mean correlation of \textbf{$r = 0.87$} ($p < 0.01$) across 28 regular seasons and detects special cases (e.g., the ``silent fan'' effect in S27). Over \textbf{85\%} of key-week estimates have confidence intervals within $\pm 15\%$, with an average certainty score of \textbf{0.78}, indicating strong stability.


\textbf{For Problem 2}, we compare rules across full seasons using the estimated votes. The \textbf{percentage-based rule} yields an average bias index \textbf{$I = 2.281$}, about 2.06 times that of the rank-based rule, and is more fan-leaning in \textbf{33/34} seasons. In controversial cases, the percentage rule lets ``high-popularity, low-skill'' contestants survive \textbf{2.1} more weeks on average and raises advancement probability by about \textbf{28\%}. Introducing a \textbf{Judge Save} mechanism reduces late-stage survival of disputed contestants by \textbf{34\%} and shifts their final ranks back by \textbf{1.8} positions, establishing a professional safeguard.


\textbf{For Problem 3}, we construct a \textbf{dual-channel random forest} to predict judges' scores and fan votes. The judge channel achieves \textbf{$R^2 = 0.68$} and is driven mainly by \textbf{partner teaching ability} and \textbf{contestant age}; the fan channel achieves \textbf{$R^2 = 0.59$} and depends more on \textbf{occupation} and \textbf{regional background}. For example, comedians receive fan-vote $z$-scores about \textbf{0.82} higher than their judge-score $z$-scores, and some highly mobilized states reach fan-vote means \textbf{1.5$\times$} above others. These results imply judges are more \textbf{ability-oriented}, while fan voting is more \textbf{structure-oriented}.


Based on these findings, we propose the \textbf{Dual-Track Linear Scoring System (DTLSS)}: a symmetric \textbf{``30+30''} scheme with judges capped at 30 points and fan scores assigned linearly by vote rank. The cap prevents excessive amplification of popularity; in stress tests, it can compress survival from a full season to \textbf{5 weeks} for extreme cases, preserving a professional floor. The system is \textbf{linear, transparent, symmetric in weights, and interpretable}.


Overall, this work provides a complete analytical framework for DWTS and similar expert-plus-public competitions, covering \textbf{data reconstruction, rule evaluation, and system design}. The approach yields actionable evidence on bias, stability, and mechanism balance, and offers a practical rule-reform option with clear fairness and interpretability advantages.

\end{sloppypar}

\vspace{1em}

\noindent\textbf{Keywords:} DWTS, vote reconstruction, Monte Carlo inversion, scoring rules, fairness evaluation

\clearpage

\pagenumbering{roman}
\setcounter{page}{1}
\rhead{Page \thepage\ of \pageref{LastRegularPage}} 
\tableofcontents

\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}
\subsection{Problem Background}


\textit{Dancing with the Stars (DWTS)} is a long-running television program that integrates professional dance competition with popular entertainment. In each season, celebrities from diverse professional backgrounds are paired with professional dancers and compete in an elimination-style format through weekly dance performances. A contestant's progression and eventual outcome in the competition are jointly determined by two core components: \textbf{professional judges' scores}, which evaluate dance technique, choreography, and artistic expression, and \textbf{audience votes}, which reflect public preferences.

Because detailed voting data are not publicly released, the core methodological challenge is to infer audience behavior from observable scores and outcomes and then evaluate how alternative rules shape fairness and competitiveness.


\subsection{Restatement of the Problem}

Because weekly fan vote data are unavailable, this study uses observable competition data (judges' scores, rankings, eliminations, and contestant profiles) to estimate relative fan votes and to evaluate how voting rules affect outcomes and fairness, with the goal of proposing a more equitable mechanism.

Specifically, the problem focuses on the following three core questions:
\begin{enumerate}
	\item \textit{Fan Vote Estimation:} In the absence of actual fan vote data, construct a model using available information such as judges' scores and competition outcomes to estimate contestants' relative fan vote levels on a weekly basis, and examine the consistency between the estimated votes and the observed elimination results.
	\item \textit{Comparison of Voting Mechanisms:} Based on the established fan vote estimation model, analyze the effects of different voting rules¡ªsuch as rank-based and percentage-based voting methods¡ªon contestants' progression paths and final competition results, and compare the differences in outcomes across these mechanisms.
	\item \textit{Evaluation of Voting Fairness:} From the perspectives of professionalism and fairness, comprehensively consider the roles of judges' scores and audience votes to assess the rationality of the current voting mechanism, and discuss its strengths and limitations in balancing competitive integrity and entertainment value.
\end{enumerate}

By addressing these tasks, the study provides quantitative evidence to inform the design of voting mechanisms in similar competitions.

\subsection{Our Work}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{Our_Work_Zoomed.png}
	\caption{Overall workflow of the study.}
	\label{fig:our_work_workflow}
\end{figure}



\section{Assumptions and Justifications}

\begin{itemize}
	\item \textbf{Assumption 1: Fan Votes are composed of ``Base Votes'' and ``Performance Votes.'' }
	
	\textit{Justification:} Votes come from loyal base fans with stable behavior and floating voters driven by weekly performance. This motivates $V_{ij} = \alpha_i P_i + \beta J_{ij}$ in Section 5.1.1.

	\item \textbf{Assumption 2: Base Popularity is constant throughout the season.}
	
	\textit{Justification:} The season is short (10--12 weeks), so core fan loyalty is relatively stable. New support is modeled as performance-related dynamics rather than shifts in base popularity, reducing parameters and enabling efficient Monte Carlo inversion.

	\item \textbf{Assumption 3: The ``Floating Vote'' pool is fixed and distributed based on Relative Performance.}
	
	\textit{Justification:} We fix the floating-vote pool at $M=5000$ for cross-season comparability. Viewers vote based on relative performance, so Section 5.1.3 maps absolute scores to relative weights, reducing judge-scale inconsistencies.

	\item \textbf{Assumption 4: Standardization eliminates cross-season biases.}
	
	\textit{Justification:} Judges differ in scoring strictness across seasons. We standardize via $z=(x-\mu)/\sigma$ to compare relative advantage and enable pooled analysis in Section 7.

\end{itemize}


\section{Notations}
\begin{center}
	\begin{tabular}{clc}
		\toprule
		\textbf{Symbol} & \textbf{Description} & \textbf{Unit} \\
		\midrule
		$i$ & Contestant index & -- \\
		$j$ & Week index & -- \\
		$s$ & Season index & -- \\
		$V_{ij}$ & Estimated fan votes of contestant $i$ in week $j$ & votes \\
		$P_i$ & Base popularity index (e.g., Google Trends normalized) & index \\
		$J_{ij}$ & Judges' score for contestant $i$ in week $j$ & points \\
		$C_{ij}$ & Cumulative performance index up to week $j$ & points \\
		$\alpha_i$ & Base-fan conversion coefficient for contestant $i$ & votes/index \\
		$\beta$ & Performance vote conversion coefficient & votes/point \\
		$M$ & Size of the floating vote pool & votes \\
		$z_{ij}$ & Standardized score: $z=(x-\mu)/\sigma$ & z-score \\
		$Score_{Judge}$ & Judge score component (0--30) & points \\
		$Score_{Fan}$ & Fan score component (0--30) & points \\
		$TotalScore$ & Overall competition score ($Score_{Judge}+Score_{Fan}$) & points \\
		$\mu_s,\sigma_s$ & Season-$s$ mean and std of judges' scores & points \\
		$r_{GT}$ & Correlation between $V$ and Google Trends & -- \\
		\bottomrule
	\end{tabular}
\end{center}

\section{Data Description and Processing}

\subsection{Data Source and Original Structure}
The raw dataset covers Seasons 1--34 of \textit{Dancing with the Stars}. It is stored in a wide format with one row per celebrity and columns for demographics (e.g., age, industry) and weekly judge scores (\texttt{weekX\_judgeY\_score}). Varying judge counts (typically 3--4) and eliminations create many NaN and zero entries that indicate non-participation.

\subsection{Data Cleaning and Feature Engineering}
To support time-series analysis and survival modeling, we convert the contestant-wide data to a contestant-week long format. The pipeline is summarized below:

\subsubsection{Data Reshaping and Filtering}
We unpivot weekly score columns so each row represents a contestant-week. Records with missing or zero scores (eliminated or non-competing) are removed to preserve data integrity.



\subsubsection{Target Variable Extraction}
The raw \texttt{results} column contains textual descriptions of the outcome (e.g., ``Eliminated Week 3'' or ``1st Place''). We parsed these strings to extract the \texttt{last\_active\_week} for each celebrity. Furthermore, we generated a binary target variable, \texttt{eliminated\_this\_week}, which takes the value 1 if the current week $t$ corresponds to the contestant's elimination week, and 0 otherwise.

After these preprocessing steps, the final dataset consists of 2,777 contestant-week observations, providing a robust foundation for the subsequent modeling of survival probabilities and score dynamics.

\section{Construction of the Stochastic Behavior Model and Vote Estimation Formula}

\subsection{Model Formulation}
To quantify missing audience votes, we construct a stochastic behavior model in which voting is driven by background popularity, current performance, and cumulative reputation. For each season, we decompose unobserved votes into four core dimensions:
\begin{itemize}
	\item \textbf{Initial Popularity Base ($\alpha_i \cdot P_i$):} Represents the baseline ``loyal fans'' the contestant brings to the competition. Their voting behavior is relatively stable and persists regardless of performance.
	\item \textbf{Weekly Performance Effect ($\beta \cdot J_{ij}$):} Captures ``floating votes'' attracted by strong weekly dance performance. The model converts judges' scores into instantaneous voting weights (the conversion method could be further improved), simulating this process.
	\item \textbf{Dynamic Accumulation Term ($\delta \cdot C_{ij}$):} Reflects the contestant's overall performance throughout the season. Sustained high-level performance may generate new ``loyal fans,'' increasing cumulative voting support.
	% \item \textbf{Environmental Random Disturbance ($\gamma_j + \varepsilon_{ij}$):} $\gamma_j$ represents weekly popularity fluctuations (e.g., finals or themed weeks), while $\varepsilon_{ij}$ captures unpredictable randomness due to voting limits or sudden controversies.
\end{itemize}

(The last term is not used in our current computation, but we can include it in the paper.)

Thus, the estimated vote formula for contestant $i$ in week $j$ is:

\begin{equation}
	V_{ij} = \alpha_i \cdot P_i + \beta \cdot J_{ij} + \delta \cdot C_{ij}
\end{equation}

\begin{table}[htbp]
	\centering
	\caption{Symbol Definitions}
	\label{tab:vote_symbol_def}
	\begin{tabular}{cl}
		\toprule
		\textbf{Symbol} & \textbf{Description} \\
		\midrule
		$V_{ij}$ & Estimated fan votes for contestant $i$ in week $j$ \\
		$P_i$ & Baseline popularity level of contestant $i$ \\
		$J_{ij}$ & Judges' score for contestant $i$ in week $j$ \\
		$C_{ij}$ & Cumulative performance of contestant $i$ through week $j$ \\
		$\alpha_i,\beta,\delta$ & Weights of the three components \\
		\bottomrule
	\end{tabular}
\end{table}

To solve the unobserved coefficients $\alpha, \beta, \delta$, we adopt Monte Carlo simulation and parameter inversion. By randomly sampling $3 \times 10^6$ parameter sets in the parameter space, we simulate elimination results under different voting rules (rank-based and percentage-based). When the simulated elimination list matches historical outcomes with Accuracy $\ge 75\%$, the parameter set is treated as a valid solution for subsequent determinacy and consistency analysis.

After obtaining a large number of valid solutions, we analyze the distribution and relationships among parameters, and define core performance metrics: Consistency and Certainty.

\subsection{Parameter Range Selection}

To avoid subjective bias, we adopt a two-stage ``wide-then-narrow'' range selection strategy.

First, based on dimensionality and sample distribution, we use a coarse symmetric range because $P_i$, $J_{ij}$, and $C_{ij}$ are on comparable scales after standardization. Thus we set $\alpha_i,\beta,\delta\in[0,3]$ to cover extreme cases of ``fan-dominant'' and ``judge-dominant.'' Then, using small-scale pilot runs, we shrink the range by retaining only intervals that produce valid elimination sequences. This yields the final sampling ranges.

The actual Monte Carlo sampling range used in this study is:
\begin{equation}
F_i \sim \mathrm{Unif}(500, 6000),\quad
\sigma_{\alpha} \sim \mathrm{Unif}(0.05, 0.30),\quad
\beta_0 \sim \mathrm{Unif}(0.5, 1.5),\quad
g \sim \mathrm{Unif}(0, 0.1).
\end{equation}
Here $F_i$ is the loyal-fan scale, $\sigma_{\alpha}$ controls fan activity volatility, $\beta_0$ is the baseline performance weight, and $g$ is the weekly growth rate. The stochastic terms are defined as:
\begin{equation}
\alpha_{ij} \sim \mathcal{N}(1,\sigma_{\alpha}),\quad
\beta_{ij} \sim \mathcal{N}\big(\beta_0(1+g\cdot(j-1)),\ 0.05\big),
\end{equation}
and a fixed floating vote pool $M=5000$ is normalized as:
\begin{equation}
P_{ij} = M \cdot \frac{w_{ij}}{\sum_k w_{kj}},\quad
w_{ij}=\begin{cases}
(N_j+1)-\mathrm{Rank}_{ij}, & \text{Rank-based} \\
\text{Score}_{ij}, & \text{Percentage-based}
\end{cases}
\end{equation}
where $N_j$ is the number of contestants remaining in week $j$.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{calibration_results_S28}
	\caption{Example: solution space of $\alpha$ and $\beta$ and estimated fan base for Season 28.}
	\label{fig:calibration_results_S28}
\end{figure}


\subsection{Consistency Metric: Reproducing Historical Eliminations}

\subsubsection{Definition of Consistency Score}
We define the consistency score $C_{score}$ as the frequency with which the predicted elimination exactly matches the actual elimination across all weeks:
\begin{equation}
	C_{score} = \frac{1}{T} \sum_{j=1}^{T} \mathbb{I}(\text{Predicted\_Eliminated}_j = \text{Actual\_Eliminated}_j)
\end{equation}
where $T$ is the number of weeks, and $\mathbb{I}(\cdot)$ is the indicator function.

\subsubsection{Consistency Check with Historical Data}
Empirically, the model reproduces most historical eliminations.

Using Monte Carlo simulation, we retain parameters with Accuracy $\ge 75\%$. Results show strong cross-season consistency, validating $V_{ij}$; distributions are shown below:

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{consistency_accuracy_heatmap}
		\caption{Heatmap of season-week consistency accuracy.}
		\label{fig:consistency_accuracy_heatmap}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{consistency_accuracy_heatmap_binary}
		\caption{Weeks with Accuracy greater than 75\%.}
		\label{fig:consistency_accuracy_heatmap_binary}
	\end{subfigure}
	\caption{Consistency heatmap and Accuracy $\approx 75\%$ week markers.}
	\label{fig:consistency_accuracy_heatmap_pair}
\end{figure}

The left plot shows season-week accuracy (darker = closer match); the right marks weeks with Accuracy $\ge 75\%$. Most seasons show high consistency, supporting the model's capture of voting behavior.

\subsubsection{External Consistency: Cross-Validation with Google Trends}
To show that $\widehat{V}_{ij}$ is not a purely mathematical fit, we use Google Trends search popularity $G_{ij}$ as an external ``public attention'' reference for reconstructed ``private votes.'' Using \texttt{pytrends}, we collect contestant trends during competition periods and compute the correlation $r$ between $\widehat{V}_{ij}$ and $G_{ij}$.

In most regular seasons, $\widehat{V}_{ij}$ and $G_{ij}$ are strongly correlated ($r \ge 0.85$), indicating alignment with real-world attention.

In special seasons (e.g., S11 and S27), the model and search trends diverge ($r_{11}=0.22$, $r_{27}=0.319$), motivating a deeper consistency discussion below.

\subsubsection{Deep Consistency Discussion: Special Cases in S11 and S27}
Despite overall alignment, Season 11 (the Bristol Palin phenomenon) and Season 27 (the Bobby Bones phenomenon) diverge between estimated votes and search trends.

\begin{itemize}
	\item \textbf{S11 controversy and attention denoising:} Bristol Palin's late-season search volume was extremely high (Figure \ref{fig:S11_divergence}), yet the model did not inflate her estimated votes, indicating it filtered non-voting attention and captured effective votes.
	\item \textbf{S27 silent fan base identification:} Bobby Bones had modest search volume but very high estimated votes (Figure \ref{fig:S27_residual}), suggesting a ``silent fan base'' of radio audiences who vote actively but rarely search.
\end{itemize}

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{S11_r.png}
		\caption{S11: model-estimated votes vs. Google Trends.}
		\label{fig:S11_divergence}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{S_27_r.png}
		\caption{S27: rank residual analysis (Bobby Bones).}
		\label{fig:S27_residual}
	\end{subfigure}
	\label{fig:special_seasons_analysis}
\end{figure}

This ``divergence'' reflects the model's ability to correct bias in external data.

\begin{itemize}
	\item For S11, the model removes ``false prosperity'' caused by controversy.
	\item For S27, the model captures ``silent fan bases'' not covered by search data.
\end{itemize}

These results show that the model not only aligns with Google Trends in normal seasons, but also reveals true voting dynamics in complex cases, demonstrating robustness in social behavior data.

\subsubsection{Summary}

By cross-validating with Google Trends, we show that the estimated fan votes $\widehat{V}_{ij}$ align with real-world attention in most seasons, demonstrating external validity.

At the same time, the model demonstrates strong denoising and identification capabilities, capturing true voting momentum rather than blindly following search volatility. This further validates the model's robustness and objectivity in complex social data.

\subsection{Certainty Metric: Stability of Estimated Fan Votes}
For the estimated fan votes, we compute statistics from $10^5$ simulations that meet the accuracy threshold $\ge 0.75$.
\begin{itemize}
	\item \textbf{Estimate stability:} We define certainty score $S_{cert}$ based on the sample variance $Var(\widehat{V}_{ij})$:
	\begin{equation}
		S_{cert}(i, j) = \frac{1}{1 + Var(\widehat{V}_{ij})}
	\end{equation}
	\item \textbf{Confidence interval:} Using normal quantile $z_{\alpha/2}$ to compute the 95\% CI:
	\begin{equation}
		CI_{95\%} = \bar{V}_{ij} \pm z_{0.025} \cdot \frac{\sigma_{ij}}{\sqrt{n_{valid}}}
	\end{equation}
\end{itemize}

We obtain weekly fan vote estimates $\widehat{V}_{ij}$ and an uncertainty measure $S_{cert}(i, j)$. These help identify which contestants have stable estimates and which exhibit large uncertainty, providing a foundation for later preference trade-off analysis.

\section{Preference Trade-Off and Anti-Volatility: Evaluating DWTS Scoring Rules}

\subsection{Research Objectives and Problem Definition}

Based on the estimated fan votes from Question 1, this section systematically compares two official aggregation rules. Our core objectives are to determine (1) how the two rules differ across seasons and their directional bias; (2) whether rule choice changes outcomes for typical controversial contestants, and whether a ``Bottom-2 Judge Save'' mechanism mitigates controversy; and (3) rule recommendations based on partiality and stability metrics.

\subsection{Metric Construction}
We input weekly judges' total scores and estimated fan votes into both aggregation rules to compute combined rankings and predicted eliminations. We then construct two metrics:

\begin{itemize}
	\item \textbf{Partiality coefficient $I$:} Measures whether final ranking is closer to fan ranking or judge ranking:
	\begin{equation}
		I = \frac{\text{Distance}(\text{Final Rank},\ \text{Judge Rank})}{\text{Distance}(\text{Final Rank},\ \text{Fan Rank})}.
	\end{equation}
	When $I>1$, final ranking is closer to fan ranking (fan-leaning). When $I<1$, it is closer to judge ranking (judge-leaning). $I=1$ indicates balance.
	\item \textbf{Stability rate $S$:} The probability that elimination results remain unchanged under small random perturbations to fan votes. This measures robustness to short-term vote noise; higher $S$ indicates more stable outcomes.
\end{itemize}

This process is equivalent to ``parallel replay'' of historical seasons to compare outcomes under the same data conditions.

\subsection{Cross-Season Comparison: Overall Differences and Partiality}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.85\textwidth]{Comparison_of_Partiality_Coefficient_(I)_Trends_for_Two_Rules_in_34_seasons.png}
	\caption{Partiality coefficient $I$ across seasons.}
	\label{fig:bias_index_compare}
\end{figure}

In Figure \ref{fig:bias_index_compare}, the blue line is the rank-based rule (mean 1.103) and the orange line is the percentage-based rule (mean 2.281); the dashed line is the $I=1$ balance baseline, and the dotted lines mark method means.

Across seasons, the percentage-based rule has higher $I$ (stronger fan bias), while the rank-based rule stays nearer 1 and is more balanced. Both rules are highly stable; differences mainly reflect bias rather than randomness.

\begin{table}[htbp]
	\centering
	\caption{Summary of cross-season comparison}
	\label{tab:rule_compare_summary}
	\renewcommand{\arraystretch}{1.15}
	\setlength{\tabcolsep}{8pt}
	\begin{tabular}{p{0.46\textwidth}cc}
		\toprule
		\textbf{Metric} & \textbf{Rank-based} & \textbf{Percentage-based} \\
		\midrule
		$I$ season mean & 1.103 & 2.281 \\
		Seasons with $I_{\text{percent}} > I_{\text{rank}}$ & 33/34  & 33/34 \\
		Counterexample season &   $I_{\text{rank}}=1.055$ & $I_{\text{percent}}=0.994$ \\
		Mean stability $S$ & 1.02 & 1.00 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent\textbf{Summary:} The percentage-based rule is more fan-biased; both rules are highly stable.

\subsection{Rule Sensitivity for Controversial Contestants and Judge Save Mechanism}

\subsubsection{Controversial Samples and Criteria}

Let contestant $i$ in week $j$ have judge rank $R^{(J)}_{ij}$ and fan rank $R^{(F)}_{ij}$. Define rank difference:
\begin{equation}
	\Delta R_{ij}=R^{(J)}_{ij}-R^{(F)}_{ij}.
\end{equation}
Define a ``conflict week'' threshold using data quantiles:
\begin{equation}
	\tau = Q_{0.90}\left(|\Delta R_{ij}|\right),\quad \mathbb{I}_{ij}=\mathbb{I}\left(|\Delta R_{ij}|\ge \tau\right).
\end{equation}
Define the number of conflict weeks in a season:
\begin{equation}
	K_i=\sum_{j=1}^{T}\mathbb{I}_{ij}.
\end{equation}
Introduce a structural conflict indicator:
\begin{equation}
	\mathbb{S}_{ij}=\mathbb{I}\left(Score_{ij}\le Q_{0.25}(Score_{\cdot j})\right)\cdot
	\mathbb{I}\left(Vote_{ij}\ge Q_{0.75}(Vote_{\cdot j})\right).
\end{equation}
Define controversial samples as
\begin{equation}
	\mathbb{C}_i=\mathbb{I}\left(K_i\ge k_0\right)\cdot \mathbb{I}\left(\sum_{j=1}^{T}\mathbb{S}_{ij}\ge 1\right),\quad k_0=2.
\end{equation}
where $Q_{p}(\cdot)$ is the $p$-quantile.


\begin{table}[htbp]
	\centering
	\caption{Definitions for controversial sample indicators}
	\label{tab:controversy_indicator_def}
	\begin{tabular}{p{0.26\textwidth} p{0.66\textwidth}}
		\toprule
		\textbf{Symbol} & \textbf{Meaning} \\
		\midrule
		$|\Delta R_{ij}|$ & Deviation between judge and fan rankings. \\
		$\tau$ & 90\% quantile of $|\Delta R_{ij}|$; defines conflict weeks. \\
		$K_i$ & Number of conflict weeks for contestant $i$. \\
		$\mathbb{S}_{ij}$ & Structural conflict: low judge scores (bottom quartile) and high fan votes (top quartile). \\
		$\mathbb{C}_i=1$ & Contestant satisfies frequent and structural conflict. \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{Controversial_Sample_Identification_Scatter_Plot.png}
	\caption{Scatter plot for controversial sample identification: conflict weeks $K_i$ vs. cumulative structural conflicts $\sum_j \mathbb{S}_{ij}$.}
	\label{fig:controversial_sample_scatter}
\end{figure}


\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.40\textwidth}
		\centering
			\includegraphics[width=\textwidth]{Heatmap_Tail_Outliers.png}
		\caption{Heatmap of $\Delta R$ outliers for controversial contestants.}
		\label{fig:delta_r_heatmap}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.40\textwidth}
		\centering
			\includegraphics[width=\textwidth]{Rank_Diff_Distribution.png}
		\caption{Distribution of rank differences $\Delta R$ across seasons.}
		\label{fig:delta_r_distribution}
	\end{subfigure}
	\caption{Conflict strength analysis: heatmap shows individual outliers; distribution shows season-level summary.}
	\label{fig:delta_r_analysis}
\end{figure}

\subsubsection{Rule Sensitivity Replay Results (Rank-based vs Percentage-based)}

For each controversial sample's season, we perform rule replay: under the same judge scores and estimated fan votes, we compute weekly combined ranks by rank-based and percentage-based rules, and record changes in final rank ($\Delta P$) and survival weeks ($\Delta W$) for controversial samples.

Aggregated replay results show that the average partiality coefficient $I$ under the percentage-based rule is 4.234, significantly higher than 1.874 for the rank-based rule. This reveals a fundamental difference: the percentage rule over-rewards fan votes, diluting judges' corrective power; samples like Bristol Palin also survive longer under the percentage rule (Table \ref{tab:rule_sensitivity_summary}).

\begin{table}[htbp]
  \centering
  \caption{Changes in final rank and survival weeks for controversial samples under two rules.}
	\begin{tabular}{cclcc@{\hspace{1em}}cclcc}
\toprule
	\multicolumn{1}{l}{Season} & \multicolumn{1}{l}{Week} & Sample & \multicolumn{1}{l}{$\Delta P$} & \multicolumn{1}{l}{$\Delta W$} &
	\multicolumn{1}{l}{Season} & \multicolumn{1}{l}{Week} & Sample & \multicolumn{1}{l}{$\Delta P$} & \multicolumn{1}{l}{$\Delta W$} \\
\midrule
	2     & 5     & Jerry Rice & -1    & -1 &
	30    & 1     & Iman Shumpert & 3     & 1 \\
	4     & 6     & Billy Ray & -2    & -3 &
	30    & 3     & Cody Rigsby & 3     & 3 \\
	11    & 2     & Bristol Palin & -2    & -1 &
	15    & 1     & Bristol Palin & 2     & 3 \\
	27    & 3     & Bobby Bones & -1    & -2 &
	32    & 2     & Mauricio Umansky & 2     & 1 \\
\bottomrule
	\end{tabular}
  \label{tab:rule_sensitivity_summary}
\end{table}

\subsubsection{Judge Save Mechanism Simulation and Impact}
By implementing a ``Bottom-2 Judge Save'' as a professional safety valve, we identify the Bottom-2 by combined rank, save the higher judge score, and eliminate the lower. This mechanism enforces a professional floor by filtering extreme fan bias and preventing low-score contestants from advancing solely on popularity, while retaining high-judge/low-fan contestants more often (Figure \ref{fig:survival_comparison_virtual_star}).

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{B_B_S27_Survival_Comparison.png}
		\caption{S27 Bobby Bones survival under three rules.}
		\label{fig:survival_comparison_bobby}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{waffles.png}
		\caption{Illustration of Judge Save impact.}
		\label{fig:judge_save_waffle}
	\end{subfigure}
	\caption{Side-by-side: survival paths and Judge Save impact.}
	\label{fig:survival_comparison_virtual_star}
\end{figure}

\subsubsection{Extreme Scenario Validation: Season Simulation Summary (S11)}

To further illustrate the mechanism, we create a hypothetical celebrity in Season 11 with consistently the lowest judge scores but the highest fan votes. Holding other contestants fixed, we simulate survival under three rules; results are shown in Figure \ref{fig:stress_test}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{Sress_Test.png}
	\caption{Stress Test: Survival Trajectories of a ``High-Popularity, Low-Skill'' Virtual Contestant under Different Rules (Season 11)}
	\label{fig:stress_test}
\end{figure}

Under both pure rank-based and percentage-based rules, this virtual star survives to the end, demonstrating the power of fan votes. When Judge Save is introduced, the contestant is eliminated by Week 5, showing the professional floor effect.

Thus, we verify the buffering role of Judge Save.

\subsubsection{Implications for Future Rules}
See below.

\section{Decoding Success: What Makes a Winning Couple?}

\subsection{Data Processing and Standardization}

We use the estimated fan votes $\widehat{V}_{ij}$ and judge scores $JudgeScore_{ij}$ from Question 1, standardized into z-scores at the season-week level to ensure comparability.

\subsection{Feature Engineering}

We build a multi-dimensional feature set covering contestant attributes, regional background, and partner teaching ability. All features are used to predict two standardized targets: \texttt{judges\_score\_z} and \texttt{fan\_votes\_z}.

\subsubsection{Contestant Attribute Features}

\begin{itemize}
	\item Continuous variables like \texttt{celebrity\_age\_during\_season} were retained.
	\item Categorical variables, such as the 30+ occupation types, were consolidated by retaining the top 10 categories by sample size and grouping the remainder into \texttt{Other} to reduce sparsity.
\end{itemize}

\subsubsection{Regional Background Features}

Regional effects may influence competition via professional resources and local fan support. We build a \textbf{three-tier regional system}:

\begin{itemize}
	\item \textbf{US state level:} For contestants with \texttt{celebrity\_homecountry/region} = ``United States'', extract \texttt{celebrity\_homestate}. Keep top 15 states; merge others as \texttt{Other US}.
    
	\item \textbf{International countries:} For non-US contestants, keep top 3 countries (e.g., England, Australia, Canada), merge others as \texttt{Other International}.
    
	\item \textbf{Final variable:} Create \texttt{Region\_Detailed} with about 20 categories (15 states + 3 countries + \texttt{Other US} + \texttt{Other International}).
\end{itemize}

This design avoids overly coarse US/non-US splitting while preventing sparse categories.

\subsubsection{Partner Ability Features}

We quantify partner effects through teaching ability, i.e., how quickly contestants improve during the season.

\begin{enumerate}
	\item \textbf{Growth trajectory modeling:} For each contestant, fit a linear regression with week as the predictor and Z-scored judge/fan scores as the response:
	\[
	Z_{\text{score}}^{(i)} = \alpha + \beta \cdot \text{Week} + \epsilon
	\]
	The slope $\beta$ is the improvement rate (\texttt{judge\_improvement\_slope}, \texttt{fan\_improvement\_slope}).
    
	\item \textbf{Partner-level aggregation:} For each partner, compute the average improvement rate and average relative performance across all assigned contestants:
	\begin{itemize}
		\item Mean improvement: $\overline{\beta}_{\text{partner}} = \frac{1}{n}\sum_{i=1}^{n} \beta_i$
		\item Mean performance: $\overline{Z}_{\text{partner}} = \frac{1}{n}\sum_{i=1}^{n} \bar{Z}_i$
	\end{itemize}
	Generate \texttt{avg\_judge\_improvement}, \texttt{avg\_fan\_improvement}, \texttt{avg\_judge\_performance}, \texttt{avg\_fan\_performance}.
\end{enumerate}

This feature system distinguishes ``high-baseline'' partners from ``coach-type'' partners.

\subsubsection{Categorical Encoding and Final Feature Matrix}

We one-hot encode \texttt{Industry\_Group} and \texttt{Region\_Detailed}, combine with continuous variables (age, season, week) to form the final feature matrix $\mathbf{X}$ (about $n \times 33$), and train two separate random forest regressors.

\subsection{Impact Analysis of Contestant Attributes}

To avoid scale differences across seasons/weeks, we use Z-scored judge and fan results. For categorical variables (occupation and region), we filter small sample categories.

\subsubsection{Occupation Effects}
Occupation analysis (Fig. 10 \& 11) reveals a clear divergence: skill-based roles (e.g., singers) excel in judge scores, while entertainers (e.g., comedians) leverage their appeal for higher fan support despite technical limits.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.40\textwidth}
		\centering
		\includegraphics[width=\textwidth]{industry_judge_impact}
		\caption{Occupation effects on judge scores (standardized).}
		\label{fig:industry_judge_impact}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.40\textwidth}
		\centering
		\includegraphics[width=\textwidth]{industry_fan_impact}
		\caption{Occupation effects on fan votes (standardized).}
		\label{fig:industry_fan_impact}
	\end{subfigure}
	\caption{Comparison of occupation effects on judge scores and fan votes.}
	\label{fig:industry_bar_impact}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{industry_scatter_optimized_with_arrow}
	\caption{Occupation impact map: judge vs fan dimensions.}
	\label{fig:industry_scatter}
\end{figure}

\subsubsection{Regional Background Effects}
Regional background exhibited a stronger influence on fan votes than on judge scores (Table \ref{tab:region_top10}). Table 5 shows a ``regional mobilization effect'' where contestants from smaller, tight-knit states (e.g., Alaska) achieve higher fan votes, suggesting local identity influences fan behavior more than professional judge assessments (Fig. 12 \& 13).

\begin{table}[htbp]
	\centering
	\caption{Top 10 regions by mean judge score and mean fan votes.}
	\label{tab:region_top10}
	\begin{tabular}{l r l r}
		\toprule
		\textbf{Region (Judges)} & \textbf{Mean Score} & \textbf{Region (Fans)} & \textbf{Mean Votes} \\
		\midrule
		USA-Minnesota & 33.42 & USA-Alaska & 7951.86 \\
		Russia & 32.95 & USA-Delaware & 7545.50 \\
		USA-Colorado & 32.92 & France & 7386.47 \\
		USA-Nevada & 30.93 & USA-Maine & 7277.06 \\
		USA-Michigan & 29.99 & USA-Mississippi & 6937.13 \\
		USA-Hawaii & 29.19 & USA-Georgia & 6863.58 \\
		Australia & 28.30 & Canada & 6806.09 \\
		France & 28.29 & USA-Iowa & 6772.70 \\
		USA-Georgia & 28.07 & USA-Hawaii & 6719.27 \\
		USA-Ohio & 27.65 & USA-Nevada & 6706.17 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{region_judges_score_violin}
	\caption{Distribution of judge scores by region (violin plot).}
	\label{fig:region_judge_violin}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{region_fan_votes_violin}
	\caption{Distribution of fan votes by region (violin plot).}
	\label{fig:region_fan_violin}
\end{figure}

\subsubsection{Age Effects}
Age shows a weak negative correlation with both judge scores ($r=-0.302$) and fan votes ($r=-0.172$), with technical assessment being slightly more sensitive (Fig. 14). This trend is likely confounded by occupation-specific age distributions (Fig. 15).

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{age_kde_lowess}
	\caption{Age vs judge scores/fan votes: KDE + Lowess trends.}
	\label{fig:age_kde_lowess}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{age_industry_interaction_v2}
	\caption{Age distributions across occupations (boxplots).}
	\label{fig:age_industry_box}
\end{figure}

For example, athletes are generally younger, while entrepreneurs and comedians are older. Yet athletes do not necessarily receive higher fan votes, indicating fans consider more than dance skill. This aligns with the occupation analysis: fan votes reflect multi-dimensional appeal rather than technical performance alone.

\subsection{Deep Modeling of Partner Effects}

Partner (professional dancer) effects include both static influence on weekly performance and dynamic teaching ability over the season. To avoid attributing contestant ability to partner skill, we decompose partner impact into two dimensions:

(1) \textbf{Base Performance}: average relative performance during the season;

(2) \textbf{Improvement Rate}: trend slope of relative performance across weeks, capturing teaching ability.

\subsubsection{Standardization and Improvement Slope}
We standardize judge scores and estimated fan votes at the season-week level:
\begin{equation}
z_{i,s,w} = \frac{x_{i,s,w}-\mu_{s,w}}{\sigma_{s,w}},\qquad x\in\{JudgeScore,\widehat{FanVote}\}.
\end{equation}
If $\sigma_{s,w}=0$ or missing, set $z_{i,s,w}=0$.

For each contestant (within a season), fit:
\begin{equation}
z_{i,s,w} = a_{i,s} + \beta_{i,s} \cdot w + \varepsilon_{i,s,w},
\end{equation}
where slope $\beta_{i,s}$ is the improvement rate (judge and fan channels). We only compute $\beta_{i,s}$ for contestants with at least 3 weeks.

At the partner level (averaging across assigned contestants), we obtain:
\begin{itemize}
	\item \textbf{Judge teaching ability:} $\overline{\beta}^{(J)}_p$ and mean judge performance $\overline{z}^{(J)}_p$;
	\item \textbf{Fan attraction ability:} $\overline{\beta}^{(F)}_p$ and mean fan performance $\overline{z}^{(F)}_p$.
\end{itemize}
To ensure stability, we keep only partners who have coached at least 5 celebrities (28 senior partners, 55 total).

Because Z-scores are relative within each week and later weeks have stronger contestants, average $\overline{\beta}$ is often negative (median about $-0.118$ for judges and $-0.107$ for fans). Thus we interpret stronger teaching as $\overline{\beta}$ closer to 0 (slower decline or even positive growth).

\subsubsection{2D Scatter: Base Performance vs Improvement Rate}
In the judge channel, base performance and improvement rate show positive correlation (about $0.330$), suggesting that partners who deliver higher relative performance also maintain competitiveness. In the fan channel, the correlation is near zero ($-0.006$), indicating that fan growth is driven more by narrative and exposure.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{partner_teaching_effect_judge}
		\caption{Judge channel: base performance (mean $z$) vs improvement rate (mean slope).}
		\label{fig:partner_teaching_judge}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{partner_growth_effect_fan}
		\caption{Fan channel: base popularity (mean $z$) vs attraction rate (mean slope).}
		\label{fig:partner_growth_fan}
	\end{subfigure}
	\caption{Senior partners in the ``base performance¡ªgrowth'' plane; point size indicates number of partners coached.}
	\label{fig:partner_scatter}
\end{figure}

\subsection{Multifactor Attribution: Random Forest Model}

To address confounding factors in univariate analysis, we employed a Dual-Channel Random Forest that models judge and fan outcomes under a unified feature mapping. Let each observation be a season-week-contestant tuple $t=(s,w,i)$ with feature vector $\mathbf{x}_t$ (age, occupation, region) and targets $y^{(J)}_t$ and $y^{(F)}_t$. We estimate
\begin{equation}
\mathbb{E}\big[y^{(\cdot)}\mid \mathbf{x}\big]\approx f^{(\cdot)}(\mathbf{x}),\qquad (\cdot)\in\{J,F\},
\end{equation}
and compare the two channels under identical inputs.

\subsubsection{Q3 Model Construction}
We trained two separate Random Forest regressors on the unified feature set to predict the season-week standardized judge scores and fan votes, respectively.

\subsubsection{Feature Importance Ranking: Judges are More ``Ability-Oriented,'' Fans More ``Structure-Oriented''}
Q3 outputs feature importance based on mean decrease in impurity (MDI). For feature $k$:
\begin{equation}
\mathrm{Imp}_k = \frac{1}{B}\sum_{b=1}^{B}\ \sum_{v\in\mathcal{V}_{b}:\ \mathrm{split}(v)=k} p(v)\,\Delta \mathrm{MSE}(v),
\end{equation}
where $\mathcal{V}_b$ is the set of split nodes, $p(v)$ is sample proportion at node $v$, and $\Delta \mathrm{MSE}(v)$ is the MSE decrease. We compare the top features across both channels.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{feature_importance_zscore}
		\caption{Random forest feature importance: predicting season-level standardized judge scores and fan votes (Z-Score).}
		\label{fig:rf_feature_importance}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{feature_importance_detailed}
		\caption{Feature importance with detailed region granularity.}
		\label{fig:rf_feature_importance_detailed}
	\end{subfigure}
	\caption{Side-by-side comparison of importances with detailed regions.}
	\label{fig:rf_feature_importance_pair}
\end{figure}

We further test whether finer regional granularity changes conclusions. If certain states rise in fan importance while judge channel remains insensitive, it supports ``regional mobilization'' in fan votes and judge neutrality.

\noindent\textbf{Section Summary:} Using a unified feature mapping $\mathbf{x}$, we fit $\hat f^{(J)}$ and $\hat f^{(F)}$ and compare MDI importances to contrast the drivers of judge vs fan channels, moving from univariate plots to multivariate structural interpretation.

\section{Recommendations to the Committee}
See ``Recommendations to the Committee (Appendix).''

\section{Strengths and Weaknesses}
\subsection{Strengths}
\begin{itemize}
	\item \textbf{Innovative method (Monte Carlo + parameter inversion + Google Trends):} We estimate fan votes via Monte Carlo inversion and validate with Google Trends, improving internal consistency, external alignment, and denoising. \textit{Improves internal validity and external alignment.}
	\item \textbf{Multi-dimensional features and dual-channel random forest:} A multi-feature system with dual-channel RF separates judge skill signals from fan structure effects. \textit{Separates judge skill signals from fan structure effects.}
	\item \textbf{30+30 dual-track linear scoring system (DTLSS):} A simple, interpretable rule that preserves audience appeal while enforcing a professional floor. \textit{Balances audience appeal and professional fairness.}
	\item \textbf{Controversial sample screening and rule replay:} Judge Save experiments quantify bias and stability with clear conclusions. \textit{Quantifies bias and stability to support conclusions.}
\end{itemize}
\subsection{Weaknesses}
\begin{itemize}
	\item \textbf{Static fan-base assumption:} We hold base fans constant, omitting time-varying shocks. \textit{May miss time-varying shocks.}
	\item \textbf{Parameter range depends on pilot runs:} ``Wide-then-narrow'' still relies on small pilots, introducing chance effects. \textit{Pilot-based ranges may introduce chance effects.}
	\item \textbf{Feature dimensions can be expanded:} No social sentiment or style-fit variables yet, limiting explanatory power. \textit{Add sentiment/engagement and style-fit variables.}
\end{itemize}

\section{Conclusion}

This paper examines the DWTS scoring tension between judges and fans. With no direct vote data, we analyze rule behavior and bias and propose improvements for fairness, interpretability, and engagement. The core finding is that controversies arise from rule amplification and should be corrected at the rule level.

Three conclusions follow: (1) judge and fan scores reflect different values, so fan influence should be constrained to avoid diluting technical evaluation; (2) both percentage and rank rules have endogenous bias and amplify extremes when popularity and skill diverge; (3) limited judge intervention or fan caps can improve fairness and stability without reducing participation.
\vspace{0.3em}
\noindent Overall conclusions:
\begin{itemize}[leftmargin=*]
	\item Judge and fan scores reflect different values; constrain fan influence to avoid diluting technical evaluation.
	\item Both percentage and rank rules have endogenous bias; when popularity and skill diverge, extreme results are amplified.
	\item Limited judge intervention or fan-score caps can improve fairness and stability without reducing participation.
\end{itemize}

Therefore, we propose a ``dual-track linear scoring system'' (30+30) that symmetrically integrates professional and popular channels, reduces implicit amplification, and improves interpretability. This approach applies to other expert-plus-public competitions; future work can expand participation and data while preserving the core structure for fairness, entertainment, and engagement.

\clearpage
\clearpage
\begin{thebibliography}{99}

\bibitem{Dangelo2018} 
S. D'Angelo, T. B. Murphy \& M. Alfo. 
``Latent Space Modeling of Multidimensional Networks with Application to the Exchange of Votes in Eurovision Song Contest.'' 
\textit{arXiv preprint arXiv:1807.06517}, 2018.

\bibitem{Blangiardo2013} 
M. Blangiardo \& G. Baio. 
``Evidence of bias in the Eurovision song contest: modelling the votes using Bayesian hierarchical models.'' 
\textit{arXiv preprint arXiv:1310.3501}, 2013.

\bibitem{Fairstein2019} 
R. Fairstein, A. Lauz, K. Gal \& R. Meir. 
``Modeling People's Voting Behavior with Poll Information.'' 
\textit{arXiv preprint arXiv:1902.04118}, 2019.

\bibitem{Chen2015} 
L. Chen, P. Xu \& D. Liu. 
``Experts versus the Crowd: A Comparison of Selection Mechanisms in Crowdsourcing Contests.'' 
\textit{SSRN Electronic Journal}, 2015. 
DOI: 10.2139/ssrn.2631317.

\bibitem{Zhang2020} 
D. Zhang. 
``Methods and Rules of Voting and Decision: A Literature Review.'' 
\textit{Open Journal of Social Sciences}, vol. 8, no. 9, pp. 310¨C326, 2020.

\bibitem{Shapley1954} 
L. S. Shapley \& M. Shubik. 
``A Method for Evaluating the Distribution of Power in a Committee System.'' 
\textit{American Political Science Review}, vol. 48, no. 3, pp. 787¨C792, 1954.

\bibitem{McDougall} 
``Voting Matters.'' 
McDougall Trust, \texttt{https://www.mcdougall.org.uk/}.

\bibitem{VotingTheory} 
``Probabilistic voting model.'' 
In \textit{Voting Theory}, \texttt{https://en.wikipedia.org/wiki/Probabilistic\_voting}.

\bibitem{GoogleTrends} 
``Google Trends.'' 
Google LLC, \texttt{https://trends.google.com/}.

\bibitem{pytrends} 
General Mills. 
``pytrends: Unofficial API for Google Trends.'' 
GitHub repository, \texttt{https://github.com/GeneralMills/pytrends}.

\end{thebibliography}
\label{LastRegularPage} 

\clearpage
\pagestyle{plain}
\pagenumbering{Roman}
\setcounter{page}{1}

\section*{Recommendations to the Committee (Appendix)}

Based on our quantitative evaluation of DWTS scoring rules (rank-based and percentage-based), we find that complex weighting is a major source of audience confusion and controversy. To enhance entertainment while protecting professional standards, we recommend a \textbf{Dual-Track Linear Scoring System (DTLSS)}.

The design philosophy is \textbf{``decoupling and balancing''}: separate professional scores and public votes into parallel tracks and linearly balance their weights.

\subsection*{New Scoring: ``30+30'' Mode}

We suggest replacing ``percentage of votes'' or ``combined ranking'' with a simple additive score. Both tracks have the same maximum (30 points):

\begin{itemize}
	\item \textbf{Judges' Track:} Unchanged. Sum of three judges' scores, maximum 30. This is the \textbf{technical ceiling}.
	\item \textbf{Fans' Track:} Convert fan vote ranking directly into points, also maximum 30. This is the \textbf{popularity ceiling}.
\end{itemize}

\subsubsection*{Computation Logic}
Fan track score $S_{Fan}$ depends only on fan ranking $Rank_{Fan}$:

\begin{equation}
	S_{Fan} = S_{max} - \delta \times (Rank_{Fan} - 1)
\end{equation}

where $S_{max} = 30$ and $\delta = 2$. Thus: rank 1 gets 30, rank 2 gets 28, and so on.

Total score is the sum:
\begin{equation}
	TotalScore = Score_{Judge} + Score_{Fan}
\end{equation}

The contestant with the lowest total score is eliminated.

\subsection*{Advantage 1: Maximum Audience Friendliness}

Unlike opaque rules where ``millions of votes offset how many judge points'' is unclear, DTLSS is highly \textbf{interpretable} and TV-friendly:

\begin{itemize}
	\item \textbf{Intuitive incentive:} ``Each rank higher in fan votes adds 2 points.'' This clear feedback loop is more motivating than complex percentages.
	\item \textbf{Visual suspense in live broadcast:} Show fixed judge scores and dynamic fan scores on screen. Viewers can see scenarios like: ``Contestant A trails by 2 points; if they outrank B in fan votes, they overtake.'' This enhances engagement.
\end{itemize}

\subsection*{Advantage 2: Structural Balance}

In Section 6.4, we show Season 27 controversy stems from unlimited fan influence under the percentage rule. The new system resolves this via \textbf{unit alignment} and explicit caps.

\subsubsection*{1. Influence Capping}
No matter how large a fan base is, the fan track benefit is capped at 30. This limits marginal returns and prevents popularity from overpowering professional scores.

\subsubsection*{2. Mathematical Defense of a Professional Floor}
Consider:
\begin{itemize}
	\item \textbf{Scenario:} Contestant X has poor technique (judge score 15) but maximum popularity (fan score 30), total $45$.
	\item \textbf{Comparison:} Contestant Y performs well (judge score 27) with moderate popularity (fan rank 3 gives 26), total $53$.
	\item \textbf{Outcome:} $53 > 45$, so Y advances.
\end{itemize}

This shows that even extreme popularity cannot fully offset low technical scores. DTLSS mathematically prevents a severely unskilled contestant from winning, ensuring fairness.

\subsection*{Summary}

DTLSS does not reduce fan engagement; it converts abstract support into visible \textbf{points}. With simple addition, it balances professional evaluation and public preference, improving fairness, interpretability, and acceptance. We recommend a pilot in Season 35.

\clearpage
\end{document}
